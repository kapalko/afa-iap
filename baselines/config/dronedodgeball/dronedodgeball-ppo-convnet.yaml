dronedodgeball-ppo-convnet:
    env: DroneDodgeBall-v0
    run: PPO
    stop:
        episode_reward_mean: 99999 # use time steps as stopping criteria
        timesteps_total: 8000000
    config:
        num_workers: 1
        framework: tf
        # environment
        env_config:
          allow_multiple_obs: true
          use_held_ball: true
          input_mode: 1
        # sgd-related
        lr: 0.0002
        lr_schedule: [[0, 0.0002], [8000000, 0.0]]
        num_sgd_iter: 8
        train_batch_size: 10240
        sgd_minibatch_size: 512
        # MDP-related
        gamma: 0.99
        rollout_fragment_length: 64
        # value function
        vf_share_layers: false
        vf_loss_coeff: 1.0
        # PPO-specific
        entropy_coeff: 0.01
        lambda: 0.95
        kl_target: 0.2
        # model
        model:
          custom_model: convnet
          custom_model_config:
            # visual observation branch
            vis_obs_shape: [256, 256, 1]
            conv_filters: [ # [out_channels, kernel_size, stride]
              [16, 8, 4], 
              [32, 4, 2]
            ]
            conv_padding: valid
            conv_activation: relu
            conv_final_activation: null
            # vector observation branch
            vec_obs_shape: [3]
            fc_filters: [32, 32]
            fc_activation: relu
            fc_final_activation: null
            # merge branch
            merge_fc_filters: [256, 256]
            merge_fc_activation: relu
            # recurrent model
            use_lstm: true
            lstm_cell_size: 32
            max_seq_len: 32