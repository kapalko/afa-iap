dronedodgeball-ppo-fcnet-no-stack-no-ball-pose:
    env: DroneDodgeBall-v0
    run: PPO
    stop:
        episode_reward_mean: 99999 # use time steps as stopping criteria
        timesteps_total: 8000000
    config:
        num_workers: 8
        num_gpus: 1
        framework: tf
        # environment
        env_config:
          no_graphics: true
          use_ball_pose: false
          use_held_ball: false
          input_mode: 3
          stacked_vec_obs: 1
        observation_filter: MeanStdFilter
        # sgd-related
        lr: 0.0002
        lr_schedule: null
        num_sgd_iter: 8
        train_batch_size: 10240
        sgd_minibatch_size: 512
        # MDP-related
        gamma: 0.99
        rollout_fragment_length: 200
        # value function
        vf_share_layers: false
        vf_loss_coeff: 1.0
        vf_clip_param: 10.0
        # PPO-specific
        entropy_coeff: 0.0
        entropy_coeff_schedule: null
        lambda: 0.95
        kl_coeff: 0.2
        kl_target: 0.01
        clip_param: 0.3
        # model
        model:
          fcnet_hiddens: [512, 256, 64, 16] # [32, 32, 16]
          fcnet_activation: relu