{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with the Air Force Arcade\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will introduce you to the Air Force Arcade and reinforcement learning by demonstrating some basic functionality of the environments, the basics of gym environments, and finally will train a small [Deep Q Network (DQN)](https://arxiv.org/pdf/1312.5602.pdf) to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: About the Environment\n",
    "\n",
    "The `DroneDodgeBall` environment places a quadcopter (the drone) in an arena with a ball launcher. \n",
    "The drone must avoid the launched balls while moving towards and staying hovering on a fixed waypoint in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image5.gif\" alt=\"Drone Dodgeball\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red circle is the waypoint were we desire the drone to remain as close as possible while remaining alive.\n",
    "\n",
    "The allowable flight boundary is a `15m x 15m x 9m` (Length x Width x Height) box.\n",
    "The quadcopter spawns at a random location within a `6m x 6m x 3m` box centered at the waypoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rewards__\n",
    "\n",
    "In the `DroneDodgeBall` environment, our rewards are a function of survival and distance to the target waypoint.\n",
    "The rewards are as follows:\n",
    "\n",
    "- reward for distance to waypoint (50Hz intervals, `x` in meters): `(1/(x+0.5)) - 0.1 clamped to the range of [0, 1]`\n",
    "- reward for survival (50Hz intervals): `0.01`\n",
    "- reward for tilting beyond 70 degrees: `-0.1`\n",
    "- reward for tilting beyond 85 degrees: `-1.0`\n",
    "- reward for being hit by a ball: `-1.0`\n",
    "- reward for exiting the game boundary: `-1.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Start the Environment\n",
    "\n",
    "First, we will import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import airforce_arcade\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create gym environment. In this step, we use OpenAI's [Gym](https://gym.openai.com/) API to relay the information from our game to Python so we can capture the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and launch the environment\n",
    "env = gym.make(\"Refueling-v0\", no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Examine the State and Action Spaces of the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of actions\n",
    "action_size = env.action_space\n",
    "print('Action Space:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env.observation_space\n",
    "print('Observations look like:', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a result that looks like:\n",
    "```\n",
    "Action Space: MultiDiscrete([3 3 3 3])\n",
    "States look like: Box(-inf, inf, (1254,), float32)\n",
    "```\n",
    "    \n",
    "So what does this mean?\n",
    "\n",
    "For our `DroneDodgeBall` environment, the drone has `4` different discrete actions it can take, with a zero, low, or high action for each.\n",
    "These correspond to roll, pitch, thrust, and yaw.\n",
    "\n",
    "For the state space, these are the observations that the drone receives. \n",
    "These are the inputs to your model and what your RL agent learns to recognize and take actions off of.\n",
    "In `DroneDodgeBall`, there are `1254` observations representing the drone's ray-based perception of its environment and a 6-vector of nomalized drone position and relative waypoint position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "gray_img = state[0].astype('float32')/255\n",
    "plt.imshow(gray_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "def preprocess(state):\n",
    "    \"\"\"Preprocess the sensor observation into a standard grayscale image and separates from attitude vector\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        obs (tuple of Box): A tuple of sensor observations type box and attitude/waypoint information type box\n",
    "    \"\"\"\n",
    "    state_img = state[0].astype('float32')/255\n",
    "    state_img = image_loader(state_img)\n",
    "    state_vec = state[1]\n",
    "    return state_img, state_vec\n",
    "\n",
    "def image_loader(state_img):\n",
    "    \"\"\"Load an image and return a tensor\"\"\"\n",
    "    loader = transforms.Compose([transforms.ToTensor()])\n",
    "    img = loader(state_img)\n",
    "    return img\n",
    "\n",
    "state_img, state_vec = preprocess(state)\n",
    "state_img[None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Take Random Actions\n",
    "\n",
    "In this next cell, we will see how the Gym API interacts with the environment by taking random actions.\n",
    "We will also explore how to receive information from the environment that will allow us to train our agents.\n",
    "\n",
    "You should be able to view the drone's behaviors in the `DroneDodgeBall` game window.\n",
    "Since the agent will be making random movements, it will look a bit erratic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "#     print('{}: {}'.format(i, action))\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(next_state[0])\n",
    "    gray_img = next_state[0].astype('float32')/255\n",
    "    print(gray_img)\n",
    "    plt.imshow(gray_img, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    next_state = np.hstack((next_state[0].reshape(-1), next_state[1].reshape(-1)))\n",
    "#     print(next_state)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train your First Model\n",
    "\n",
    "Unity can sometimes be finicky.\n",
    "If you can't get training to start, or it seems that the notebook is lagging, restart your kernel and run steps 0-2.\n",
    "\n",
    "![Kernel Restart](images/kernel_restart.png)\n",
    "\n",
    "__Note:__ These environments are advanced and will require significant training.\n",
    "The code provided demonstrates the basics of training a model but will most likely not train a highly sophisticated autonomous agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training a Deep Q-Network__\n",
    "\n",
    "This starter code runs for only 50 episodes. \n",
    "On a local computer, this can take between 5-10 minutes.\n",
    "You may notice that the agent doesn't really learn much.\n",
    "In fact, our training required several thousand training episodes to begin to learn appropriate behaviors.\n",
    "\n",
    "There are two supporting files required to run this code:\n",
    "- `dqn_agent.py` defines the functions of the agent, including how to select actions, take steps, and save experiences for your networks to train on\n",
    "- `model.py` is the defined architecture of the neural network. In this example, it's a small 3 layer network with 64 hidden nodes per layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_obs(obs):\n",
    "    return np.hstack((obs[0].reshape(-1), obs[1].reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(obs):\n",
    "#     \"\"\"Preprocess the sensor observation into a standard grayscale image and separates from attitude vector\n",
    "    \n",
    "#     Params\n",
    "#     ======\n",
    "#         obs (tuple of Box): A tuple of sensor observations type box and attitude/waypoint information type box\n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     return obs[0].astype('float32')/255, obs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual State Size: (19, 33)\n",
      "Vector State Size: 11\n",
      "Action_size: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylepalko/anaconda3/envs/csrp/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n",
      "2021-07-06 22:30:45 INFO [environment.py:112] Connected to Unity environment with package version 1.9.0-preview and communication version 1.5.0\n",
      "2021-07-06 22:30:46 INFO [environment.py:282] Connected new brain:\n",
      "FighterBehavior?team=0\n",
      "2021-07-06 22:30:46 WARNING [gym_unity.py:91] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n",
      "/Users/kylepalko/anaconda3/envs/csrp/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/kylepalko/anaconda3/envs/csrp/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df980377f536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# plot the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-df980377f536>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# next_state = reshape_obs(next_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Palko Working/MIT AI/113_21CSRP/afa-iap/examples/CNN/dqn_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# If enough samples are available in memory, get random subset and learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Palko Working/MIT AI/113_21CSRP/afa-iap/examples/CNN/dqn_agent.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import airforce_arcade\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from dqn_agent import Agent\n",
    "agent = Agent(viz_state_size=(19,33), vec_state_size=11, action_size=81, seed=42)\n",
    "\n",
    "# create and launch the environment\n",
    "env = gym.make(\"Refueling-v0\", no_graphics=True)\n",
    "\n",
    "def dqn(n_episodes=1000, max_t=10000, eps_start=1.0, eps_end=0.01, eps_decay=0.999):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        # state = reshape_obs(state)\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            action_map = agent.act_map[action]  # we map our return from a single value to our multidiscrete action\n",
    "            next_state, reward, done, info = env.step(action_map)\n",
    "            # next_state = reshape_obs(next_state)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "#         print('\\rEpisode {}\\tAverage Score: {:.2f}\\n'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoints/checkpoint_local.pth')\n",
    "            torch.save(agent.qnetwork_target.state_dict(), 'checkpoints/checkpoint_target.pth')\n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoints/checkpoint_local.pth')\n",
    "    torch.save(agent.qnetwork_target.state_dict(), 'checkpoints/checkpoint_target.pth')\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and launch the environment\n",
    "env = gym.make(\"Refueling-v0\", no_graphics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can watch how your agent performs here\n",
    "## Just ensure your environment is set to no_graphics=False\n",
    "\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoints/checkpoint_local.pth'))\n",
    "agent.qnetwork_target.load_state_dict(torch.load('checkpoints/checkpoint_target.pth'))\n",
    "\n",
    "scores = []\n",
    "n_test_episodes = 10\n",
    "for i in range(n_test_episodes):\n",
    "    state = env.reset()\n",
    "    state = reshape_obs(state)\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        print(agent.act_map[action])\n",
    "        state, reward, done, _ = env.step(agent.act_map[action])\n",
    "        state = reshape_obs(state)\n",
    "        score += reward\n",
    "    scores.append(score)\n",
    "\n",
    "print('Average score over {} episodes is: {}'.format(n_test_episodes, round(sum(scores)/n_test_episodes, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 22:32:01 INFO [environment.py:429] Environment shut down with return code 0.\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Thoughts\n",
    "\n",
    "To train a well performing model, we most likely need to increase the number of episodes, `n_episodes`, and also increase the number of layers and nodes within each.\n",
    "Furthermore, while DQN was one of the first deep RL algorithms to solve Atari, several advances in the field of RL have developed novel algorithms that outperform DQN.\n",
    "Several of these algorithms are already tailored for use within the RLlib package.\n",
    "\n",
    "`DroneDodgeBall` is a complex environment and will take significant time to solve.\n",
    "Do not be discouraged if you are not getting good training results in a short period of time.\n",
    "\n",
    "Below is an example of a larger model training for a longer period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example Results of a Well Trained Model__\n",
    "\n",
    "The following graph is a training log of a policy-based agent trained over 8 million steps (~60,000 episodes).\n",
    "This training took about 16 hours on the MIT supercomputer.\n",
    "\n",
    "![ray_learned](images/ray_learned.png)\n",
    "\n",
    "It is important to remember that reinforcement learning alogrithms can take an extremely long time to train.\n",
    "DeepMind AlphaStar, one of the most advanced RL algorithms that learned to play Starcraft, took over 46 years worth of training time on TPUv3s, some of the most powerful processors on the planet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csrp1",
   "language": "python",
   "name": "csrp1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
